{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# liaisons-experiments - Framework Try-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from liaisons_experiments.experiments.multi_experiment import MultiExperiment\n",
    "from tqdm.notebook import tqdm\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "import os\n",
    "\n",
    "self_hosted_llms = [\n",
    "    (ChatOllama(\n",
    "        model=\"llama3:8b\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=2,\n",
    "        top_p=1,\n",
    "    ), {\n",
    "        \"num_workers\": 5\n",
    "    }),\n",
    "    (ChatOllama(\n",
    "        model=\"phi3:3.8b\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=2,\n",
    "        top_p=1,\n",
    "    ), {\n",
    "        \"num_workers\": 10\n",
    "    }),\n",
    "    (ChatOllama(\n",
    "        model=\"phi3:14b\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=2,\n",
    "        top_p=1,\n",
    "    ), {\n",
    "        \"num_workers\": 3\n",
    "    }),\n",
    "    (ChatOllama(\n",
    "        model=\"gemma:2b\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=2,\n",
    "        top_p=1,\n",
    "    ), {\n",
    "        \"num_workers\": 10\n",
    "    }),\n",
    "    (ChatOllama(\n",
    "        model=\"gemma:2b-it\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=2,\n",
    "        top_p=1,\n",
    "    ), {\n",
    "        \"num_workers\": 10\n",
    "    }),\n",
    "    (ChatOllama(\n",
    "        model=\"gemma:7b\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=2,\n",
    "        top_p=1,\n",
    "    ), {\n",
    "        \"num_workers\": 5\n",
    "    }),\n",
    "    (ChatOllama(\n",
    "        model=\"gemma:7b-it\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=2,\n",
    "        top_p=1,\n",
    "    ), {\n",
    "        \"num_workers\": 5\n",
    "    }),\n",
    "    (ChatOllama(\n",
    "        model=\"gemma2:2b\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=2,\n",
    "        top_p=1,\n",
    "    ), {\n",
    "        \"num_workers\": 10\n",
    "    }),\n",
    "    (ChatOllama(\n",
    "        model=\"gemma2:2b-it\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=2,\n",
    "        top_p=1,\n",
    "    ), {\n",
    "        \"num_workers\": 10\n",
    "    }),\n",
    "    (ChatOllama(\n",
    "        model=\"gemma2:9b\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=2,\n",
    "        top_p=1,\n",
    "    ), {\n",
    "        \"num_workers\": 5\n",
    "    }),\n",
    "    (ChatOllama(\n",
    "        model=\"gemma2:9b-it\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=2,\n",
    "        top_p=1,\n",
    "    ), {\n",
    "        \"num_workers\": 5\n",
    "    }),\n",
    "    (ChatOllama(\n",
    "        model=\"gemma2:27b\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=2,\n",
    "        top_p=1,\n",
    "    ), {\n",
    "        \"num_workers\": 2\n",
    "    }),\n",
    "    (ChatOllama(\n",
    "        model=\"gemma2:27b-it\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=2,\n",
    "        top_p=1,\n",
    "    ), {\n",
    "        \"num_workers\": 2\n",
    "    }),\n",
    "]\n",
    "\n",
    "platform_hosted_llms = [\n",
    "    (ChatOpenAI(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=2,\n",
    "        top_p=1,\n",
    "        api_key=os.environ[\"LIAISONS_EXPERIMENTS_OPENAI_API_KEY\"],\n",
    "    ), {\n",
    "        \"num_workers\": 16,\n",
    "    }),\n",
    "    (ChatOpenAI(\n",
    "        model=\"gpt-4-turbo-2024-04-09\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=2,\n",
    "        top_p=1,\n",
    "        api_key=os.environ[\"LIAISONS_EXPERIMENTS_OPENAI_API_KEY\"],\n",
    "    ), {\n",
    "        \"num_workers\": 1,\n",
    "    }),\n",
    "    (ChatOpenAI(\n",
    "        model=\"gpt-4o-2024-05-13\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=2,\n",
    "        top_p=1,\n",
    "        api_key=os.environ[\"LIAISONS_EXPERIMENTS_OPENAI_API_KEY\"],\n",
    "    ), {\n",
    "        \"num_workers\": 1,\n",
    "    }),\n",
    "    (ChatOpenAI(\n",
    "        model=\"gpt-4o-mini-2024-07-18\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=2,\n",
    "        top_p=1,\n",
    "        api_key=os.environ[\"LIAISONS_EXPERIMENTS_OPENAI_API_KEY\"],\n",
    "    ), {\n",
    "        \"num_workers\": 16,\n",
    "    }),\n",
    "    (ChatAnthropic(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=2,\n",
    "        top_p=1,\n",
    "        api_key=os.environ[\"LIAISONS_EXPERIMENTS_ANTHROPIC_API_KEY\"],\n",
    "    ),{\n",
    "        \"num_workers\": 2,\n",
    "    }),\n",
    "    (ChatAnthropic(\n",
    "        model=\"claude-3-sonnet-20240229\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=2,\n",
    "        top_p=1,\n",
    "        api_key=os.environ[\"LIAISONS_EXPERIMENTS_ANTHROPIC_API_KEY\"],\n",
    "    ),{\n",
    "        \"num_workers\": 2,\n",
    "    }),\n",
    "    (ChatAnthropic(\n",
    "        model=\"claude-3-opus-20240229\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=2,\n",
    "        top_p=1,\n",
    "        api_key=os.environ[\"LIAISONS_EXPERIMENTS_ANTHROPIC_API_KEY\"],\n",
    "    ),{\n",
    "        \"num_workers\": 1,\n",
    "    }),\n",
    "    (ChatAnthropic(\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=2,\n",
    "        top_p=1,\n",
    "        api_key=os.environ[\"LIAISONS_EXPERIMENTS_ANTHROPIC_API_KEY\"],\n",
    "    ),{\n",
    "        \"num_workers\": 2,\n",
    "    }),\n",
    "    (GoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        temperature=0.7,\n",
    "        max_output_tokens=2,\n",
    "        top_p=1,\n",
    "        google_api_key=os.environ[\"LIAISONS_EXPERIMENTS_GOOGLE_API_KEY\"],\n",
    "    ),{\n",
    "        \"num_workers\": 16,\n",
    "    }),\n",
    "    (GoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-pro\",\n",
    "        temperature=0.7,\n",
    "        max_output_tokens=2,\n",
    "        top_p=1,\n",
    "        google_api_key=os.environ[\"LIAISONS_EXPERIMENTS_GOOGLE_API_KEY\"],\n",
    "    ),{\n",
    "        \"num_workers\": 16,\n",
    "    }),\n",
    "]\n",
    "\n",
    "exps = MultiExperiment(self_hosted_llms, tqdm=tqdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "hf_token = os.environ.get(\"LIAISONS_HUGGING_FACE_API_KEY\")\n",
    "\n",
    "dataset = load_dataset(\"coding-kelps/liaisons-claim-stance-sample\", token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_binary_results(binary_results, title: str | None = None):\n",
    "    binary_plot_results = pd.merge(binary_results.f1_scores, binary_results.metadata) \\\n",
    "        .melt(id_vars='model_name', var_name='Metric', value_name='Value')\n",
    "\n",
    "    # Set the size of the plot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    # Define a list of colors for the palette\n",
    "    colors = [\"#1F77B4\", \"#FF7F0F\", \"#2BA02B\", \"#D62727\"]\n",
    "\n",
    "    # Create a grouped bar plot\n",
    "    ax = sns.barplot(data=binary_plot_results, x='model_name', y='Value', hue='Metric', palette=colors)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Model Name\")\n",
    "    plt.ylabel(\"Benchmarks\")\n",
    "\n",
    "    # Fix ticks position to avoid hazardous position\n",
    "    # https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.set_xticklabels.html\n",
    "    ax.set_xticks(ax.get_xticks())\n",
    "    # Rotate labels and align to the right\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_ternary_results(ternary_results, title: str | None = None):\n",
    "    ternary_plot_results = pd.merge(ternary_results.f1_scores, ternary_results.metadata) \\\n",
    "        .melt(id_vars='model_name', var_name='Metric', value_name='Value')\n",
    "    \n",
    "    # Set the size of the plot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    \n",
    "    # Define a list of colors for the palette\n",
    "    colors = [\"#1F77B4\", \"#FF7F0F\", \"#9467BD\", \"#2BA02B\", \"#D62727\"]\n",
    "    \n",
    "    # Create a grouped bar plot\n",
    "    ax = sns.barplot(data=ternary_plot_results, x='model_name', y='Value', hue='Metric', palette=colors)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Model Name\")\n",
    "    plt.ylabel(\"Benchmarks\")\n",
    "    \n",
    "    # Fix ticks position to avoid hazardous position\n",
    "    # https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.set_xticklabels.html\n",
    "    ax.set_xticks(ax.get_xticks())\n",
    "    # Rotate labels and align to the right\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_ibm_few_shot_prompting(parent_argment: str, child_argument: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "Arg1: Even in the case of provocateurs, it can be an effective strategy to call their bluff, by offering them a chance to have a rational conversation. In this case, the failure to do so is their responsibility alone.\n",
    "Arg2: No-platforming hinders productive discourse.\n",
    "Relation: attack\n",
    "\n",
    "Arg1: A country used to receiving ODA may be perpetually bound to depend on handouts (pp. 197).\n",
    "Arg2: Government structures adapt to handle and distribute incoming ODA. As the funding from ODA is significant, countries have vested bureaucratic interest to remain bound to aid (pp. 197).\n",
    "Relation: support\n",
    "\n",
    "Arg1: Elections would limit the influence of lobbyists on the appointment of Supreme Court judges.\n",
    "Arg2: The more individuals take part in a decision, as would be the case in a popular vote compared to a vote in the Senate, the harder it is to sway the outcome.\n",
    "Relation: support\n",
    "\n",
    "Arg1: ChatGPT will reach AGI level before 2030.\n",
    "Arg2: To reach AGI it should be able to generate its own goals and intentions: where would it draw these from?\n",
    "Relation: attack\n",
    "\n",
    "Arg1: {parent_argment}\n",
    "Arg2: {child_argument}\n",
    "Relation: \n",
    "\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def binary_ibm_augmented_few_shot_prompting(parent_argment: str, child_argument: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "Arg1: Even in the case of provocateurs, it can be an effective strategy to call their bluff, by offering them a chance to have a rational conversation. In this case, the failure to do so is their responsibility alone.\n",
    "Arg2: No-platforming hinders productive discourse.\n",
    "Relation: attack\n",
    "\n",
    "Arg1: A country used to receiving ODA may be perpetually bound to depend on handouts (pp. 197).\n",
    "Arg2: Government structures adapt to handle and distribute incoming ODA. As the funding from ODA is significant, countries have vested bureaucratic interest to remain bound to aid (pp. 197).\n",
    "Relation: support\n",
    "\n",
    "Arg1: Elections would limit the influence of lobbyists on the appointment of Supreme Court judges.\n",
    "Arg2: The more individuals take part in a decision, as would be the case in a popular vote compared to a vote in the Senate, the harder it is to sway the outcome.\n",
    "Relation: support\n",
    "\n",
    "Arg1: ChatGPT will reach AGI level before 2030.\n",
    "Arg2: To reach AGI it should be able to generate its own goals and intentions: where would it draw these from?\n",
    "Relation: attack\n",
    "\n",
    "---\n",
    "\n",
    "What the relation between Arg1 and Arg2, respond with one word: support or attack:\n",
    "\n",
    "Arg1: {parent_argment}\n",
    "Arg2: {child_argument}\n",
    "Relation: \n",
    "\"\"\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_df = dataset['binary'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_binary_results = exps.run_from_df(binary_df, binary_ibm_few_shot_prompting, relation_dim=\"binary\")\n",
    "\n",
    "plot_binary_results(few_shot_binary_results, title=\"Large Language Models for binary argumentative relation prediction over the IBM Debater preprocessed dataset sample using few shot prompting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_few_shot_binary_results = exps.run_from_df(binary_df, binary_ibm_augmented_few_shot_prompting, relation_dim=\"binary\")\n",
    "\n",
    "plot_binary_results(augmented_few_shot_binary_results, title=\"Large Language Models for binary argumentative relation prediction over the IBM Debater preprocessed dataset sample using few augmented shot prompting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ternary_ibm_few_shot_prompting(parent_argment: str, child_argument: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "Arg1: Even in the case of provocateurs, it can be an effective strategy to call their bluff, by offering them a chance to have a rational conversation. In this case, the failure to do so is their responsibility alone.\n",
    "Arg2: No-platforming hinders productive discourse.\n",
    "Relation: attack\n",
    "\n",
    "Arg1: ChatGPT will reach AGI level before 2030.\n",
    "Arg2: Government structures adapt to handle and distribute incoming ODA. As the funding from ODA is significant, countries have vested bureaucratic interest to remain bound to aid (pp. 197).\n",
    "Relation: unrelated\n",
    "\n",
    "Arg1: Elections would limit the influence of lobbyists on the appointment of Supreme Court judges.\n",
    "Arg2: The more individuals take part in a decision, as would be the case in a popular vote compared to a vote in the Senate, the harder it is to sway the outcome.\n",
    "Relation: support\n",
    "\n",
    "Arg1: A country used to receiving ODA may be perpetually bound to depend on handouts (pp. 197).\n",
    "Arg2: To reach AGI it should be able to generate its own goals and intentions: where would it draw these from?\n",
    "Relation: unrelated\n",
    "\n",
    "Arg1: {parent_argment}\n",
    "Arg2: {child_argument}\n",
    "Relation: \n",
    "\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def ternary_ibm_augmented_few_shot_prompting(parent_argment: str, child_argument: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "Arg1: Even in the case of provocateurs, it can be an effective strategy to call their bluff, by offering them a chance to have a rational conversation. In this case, the failure to do so is their responsibility alone.\n",
    "Arg2: No-platforming hinders productive discourse.\n",
    "Relation: attack\n",
    "\n",
    "Arg1: ChatGPT will reach AGI level before 2030.\n",
    "Arg2: Government structures adapt to handle and distribute incoming ODA. As the funding from ODA is significant, countries have vested bureaucratic interest to remain bound to aid (pp. 197).\n",
    "Relation: unrelated\n",
    "\n",
    "Arg1: Elections would limit the influence of lobbyists on the appointment of Supreme Court judges.\n",
    "Arg2: The more individuals take part in a decision, as would be the case in a popular vote compared to a vote in the Senate, the harder it is to sway the outcome.\n",
    "Relation: support\n",
    "\n",
    "Arg1: A country used to receiving ODA may be perpetually bound to depend on handouts (pp. 197).\n",
    "Arg2: To reach AGI it should be able to generate its own goals and intentions: where would it draw these from?\n",
    "Relation: unrelated\n",
    "\n",
    "---\n",
    "\n",
    "What the relation between Arg1 and Arg2, respond with one word: support, attack, or unrelated:\n",
    "\n",
    "Arg1: {parent_argment}\n",
    "Arg2: {child_argument}\n",
    "Relation: \n",
    "\"\"\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ternary_df = dataset['ternary'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_ternary_results = exps.run_from_df(ternary_df, ternary_ibm_few_shot_prompting, relation_dim=\"ternary\")\n",
    "\n",
    "plot_ternary_results(few_shot_ternary_results, title=\"Large Language Models for ternary argumentative relation prediction over the IBM Debater preprocessed dataset sample using few shot prompting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_few_shot_ternary_results = exps.run_from_df(ternary_df, ternary_ibm_augmented_few_shot_prompting, relation_dim=\"ternary\")\n",
    "\n",
    "plot_ternary_results(augmented_few_shot_ternary_results, title=\"Large Language Models for ternary argumentative relation prediction over the IBM Debater preprocessed dataset sample using augmented few shot prompting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_f1 = pd.concat([augmented_few_shot_binary_results.f1_scores.assign(prompting=\"augmented_few_shot\"), few_shot_binary_results.f1_scores.assign(prompting=\"few_shot\")])\n",
    "\n",
    "binary_metadata = pd.concat([augmented_few_shot_binary_results.metadata, few_shot_binary_results.metadata])\n",
    "\n",
    "binary_results = binary_f1.merge(binary_metadata)\n",
    "\n",
    "binary_results.to_csv(\"binary_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ternary_f1 = pd.concat([augmented_few_shot_ternary_results.f1_scores.assign(prompting=\"augmented_few_shot\"), few_shot_ternary_results.f1_scores.assign(prompting=\"few_shot\")])\n",
    "\n",
    "ternary_metadata = pd.concat([augmented_few_shot_ternary_results.metadata, few_shot_ternary_results.metadata])\n",
    "\n",
    "ternary_results = ternary_f1.merge(ternary_metadata)\n",
    "\n",
    "ternary_results.to_csv(\"ternary_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
