{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# liaisons-experiments - Framework Try-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from liaisons_experiments.experiments.multi_experiment import MultiExperiment\n",
    "from tqdm.notebook import tqdm\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "import os\n",
    "\n",
    "llms = [\n",
    "    ChatOllama(\n",
    "        model=\"gemma:2b\",\n",
    "        temperature=0.7,\n",
    "        num_predict=2,\n",
    "        top_p=1,\n",
    "    ),\n",
    "    ChatOllama(\n",
    "        model=\"gemma:2b-it\",\n",
    "        temperature=0.7,\n",
    "        num_predict=2,\n",
    "        top_p=1,\n",
    "    ),\n",
    "    ChatOllama(\n",
    "        model=\"gemma:7b\",\n",
    "        temperature=0.7,\n",
    "        num_predict=2,\n",
    "        top_p=1,\n",
    "    ),\n",
    "    ChatOllama(\n",
    "        model=\"gemma:7b-it\",\n",
    "        temperature=0.7,\n",
    "        num_predict=2,\n",
    "        top_p=1,\n",
    "    ),\n",
    "    ChatOllama(\n",
    "        model=\"gemma2:2b\",\n",
    "        temperature=0.7,\n",
    "        num_predict=2,\n",
    "        top_p=1,\n",
    "    ),\n",
    "    ChatOllama(\n",
    "        model=\"gemma2:2b-it\",\n",
    "        temperature=0.7,\n",
    "        num_predict=2,\n",
    "        top_p=1,\n",
    "    ),\n",
    "    ChatOllama(\n",
    "        model=\"gemma2:9b\",\n",
    "        temperature=0.7,\n",
    "        num_predict=2,\n",
    "        top_p=1,\n",
    "    ),\n",
    "    ChatOllama(\n",
    "        model=\"gemma2:9b-it\",\n",
    "        temperature=0.7,\n",
    "        num_predict=2,\n",
    "        top_p=1,\n",
    "    ),\n",
    "    ChatOllama(\n",
    "        model=\"gemma2:27b\",\n",
    "        temperature=0.7,\n",
    "        num_predict=2,\n",
    "        top_p=1,\n",
    "    ),\n",
    "    ChatOllama(\n",
    "        model=\"gemma2:27b-it\",\n",
    "        temperature=0.7,\n",
    "        num_predict=2,\n",
    "        top_p=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "exps = MultiExperiment(llms, output_dir=\".\", tqdm=tqdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "hf_token = os.environ.get(\"LIAISONS_HUGGING_FACE_API_KEY\")\n",
    "\n",
    "dataset = load_dataset(\"coding-kelps/liaisons-claim-stance-sample\", token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_ibm_few_shot_prompting(parent_argment: str, child_argument: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "Arg1: Even in the case of provocateurs, it can be an effective strategy to call their bluff, by offering them a chance to have a rational conversation. In this case, the failure to do so is their responsibility alone.\n",
    "Arg2: No-platforming hinders productive discourse.\n",
    "Relation: attack\n",
    "\n",
    "Arg1: A country used to receiving ODA may be perpetually bound to depend on handouts (pp. 197).\n",
    "Arg2: Government structures adapt to handle and distribute incoming ODA. As the funding from ODA is significant, countries have vested bureaucratic interest to remain bound to aid (pp. 197).\n",
    "Relation: support\n",
    "\n",
    "Arg1: Elections would limit the influence of lobbyists on the appointment of Supreme Court judges.\n",
    "Arg2: The more individuals take part in a decision, as would be the case in a popular vote compared to a vote in the Senate, the harder it is to sway the outcome.\n",
    "Relation: support\n",
    "\n",
    "Arg1: ChatGPT will reach AGI level before 2030.\n",
    "Arg2: To reach AGI it should be able to generate its own goals and intentions: where would it draw these from?\n",
    "Relation: attack\n",
    "\n",
    "Arg1: {parent_argment}\n",
    "Arg2: {child_argument}\n",
    "Relation: \n",
    "\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "binary_df = dataset['binary'].to_pandas()\n",
    "\n",
    "binary_results = exps.run_from_df(binary_df, binary_ibm_few_shot_prompting, relation_dim=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "binary_plot_results = pd.merge(binary_results.f1_scores, binary_results.metadata) \\\n",
    "    .melt(id_vars='model_name', var_name='Metric', value_name='Value')\n",
    "\n",
    "# Set the size of the plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Define a list of colors for the palette\n",
    "colors = [\"#1F77B4\", \"#FF7F0F\", \"#2BA02B\", \"#D62727\"]\n",
    "\n",
    "# Create a grouped bar plot\n",
    "ax = sns.barplot(data=binary_plot_results, x='model_name', y='Value', hue='Metric', palette=colors)\n",
    "\n",
    "plt.title(\"Large Language Models for binary argumentative prediction over the IBM Debater preprocessed dataset sample\")\n",
    "plt.xlabel(\"Model Name\")\n",
    "plt.ylabel(\"Benchmarks\")\n",
    "\n",
    "# Fix ticks position to avoid hazardous position\n",
    "# https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.set_xticklabels.html\n",
    "ax.set_xticks(ax.get_xticks())\n",
    "# Rotate labels and align to the right\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ternary_ibm_few_shot_prompting(parent_argment: str, child_argument: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "Arg1: Even in the case of provocateurs, it can be an effective strategy to call their bluff, by offering them a chance to have a rational conversation. In this case, the failure to do so is their responsibility alone.\n",
    "Arg2: No-platforming hinders productive discourse.\n",
    "Relation: attack\n",
    "\n",
    "Arg1: ChatGPT will reach AGI level before 2030.\n",
    "Arg2: Government structures adapt to handle and distribute incoming ODA. As the funding from ODA is significant, countries have vested bureaucratic interest to remain bound to aid (pp. 197).\n",
    "Relation: unrelated\n",
    "\n",
    "Arg1: Elections would limit the influence of lobbyists on the appointment of Supreme Court judges.\n",
    "Arg2: The more individuals take part in a decision, as would be the case in a popular vote compared to a vote in the Senate, the harder it is to sway the outcome.\n",
    "Relation: support\n",
    "\n",
    "Arg1: A country used to receiving ODA may be perpetually bound to depend on handouts (pp. 197).\n",
    "Arg2: To reach AGI it should be able to generate its own goals and intentions: where would it draw these from?\n",
    "Relation: unrelated\n",
    "\n",
    "Arg1: {parent_argment}\n",
    "Arg2: {child_argument}\n",
    "Relation: \n",
    "\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "ternary_df = dataset['ternary'].to_pandas()\n",
    "\n",
    "ternary_results = exps.run_from_df(ternary_df, ternary_ibm_few_shot_prompting, relation_dim=\"ternary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ternary_plot_results = pd.merge(ternary_results.f1_scores, ternary_results.metadata) \\\n",
    "    .melt(id_vars='model_name', var_name='Metric', value_name='Value')\n",
    "\n",
    "# Set the size of the plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "\n",
    "# Define a list of colors for the palette\n",
    "colors = [\"#1F77B4\", \"#FF7F0F\", \"#9467BD\", \"#2BA02B\", \"#D62727\"]\n",
    "\n",
    "# Create a grouped bar plot\n",
    "ax = sns.barplot(data=ternary_plot_results, x='model_name', y='Value', hue='Metric', palette=colors)\n",
    "\n",
    "plt.title(\"Large Language Models for ternary argumentative prediction over the IBM Debater preprocessed dataset sample\")\n",
    "plt.xlabel(\"Model Name\")\n",
    "plt.ylabel(\"Benchmarks\")\n",
    "\n",
    "# Fix ticks position to avoid hazardous position\n",
    "# https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.set_xticklabels.html\n",
    "ax.set_xticks(ax.get_xticks())\n",
    "# Rotate labels and align to the right\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
